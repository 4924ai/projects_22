{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMHT8vv5GkTO"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow==1.15.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "s3CBRzmyHY1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_parameter = 0.01\n",
        "epochs = 300"
      ],
      "metadata": {
        "id": "92RuNtNcHbU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_points=50\n",
        "x_train=np.linspace(0,30,sample_points)\n",
        "y_train=6*x_train+7*np.random.randn(sample_points)"
      ],
      "metadata": {
        "id": "ILLsq9dWHdtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x_train, y_train, 'o')\n",
        "plt.plot(x_train, 6*x_train)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R0y50HbYHgPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = tf.placeholder(tf.float32)\n",
        "X = tf.placeholder(tf.float32)\n",
        "\n",
        "W=tf.Variable(np.random.randn(), name='weights')\n",
        "B=tf.Variable(np.random.randn(), name='bias')"
      ],
      "metadata": {
        "id": "zvBM9Ow7HiPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction=W*X+B\n",
        "cost_iteration=tf.reduce_sum((prediction-Y)**2)/(2*sample_points)\n",
        "optimizer=tf.train.GradientDescentOptimizer(learning_parameter).minimize(cost_iteration)\n",
        "init=tf.global_variables_initializer()"
      ],
      "metadata": {
        "id": "Z4gXdYeRHmSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  for epoch in range(epochs):\n",
        "    for x, y in zip(x_train, y_train):\n",
        "      if not epoch%40:\n",
        "        W1=sess.run(W)\n",
        "        B1=sess.run(B)\n",
        "        cost_iter=sess.run(cost_iteration, feed_dict={X:x, Y:y})\n",
        "        print('Epochs %f Cost %f Weight %f Bias %f' %(epoch, cost_iter, W1, B1))\n",
        "  Weight=sess.run(W)\n",
        "  Bias=sess.run(B)\n",
        "\n",
        "  plt.plot(x_train, y_train, 'o')\n",
        "  plt.plot(x_train, Weight*x_train+Bias)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "oyqvl-3PHpHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"Model\") as scope:\n",
        "  prediction=W*X+Bias\n",
        "  weight_histogram=tf.summary.histogram(\"Weights\", W)\n",
        "  bias_histogram=tf.summary.histogram(\"Bias\", B)\n",
        "\n",
        "with tf.name_scope(\"Cost_function\") as scope:\n",
        "  cost_iteration=tf.reduce_sum((prediction-Y)**2)/(2*sample_points)\n",
        "\n",
        "cost_summary=tf.summary.scalar(\"Cost\", cost_iteration)"
      ],
      "metadata": {
        "id": "dpkScUrrHrPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"Training\") as scope:\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_parameter).minimize(cost_iteration)\n",
        "  \n",
        "  init=tf.global_variables_initializer()\n",
        "\n",
        "  merged_summary=tf.summary.merge_all()"
      ],
      "metadata": {
        "id": "fQth0dtiIGeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  writer=tf.summary.FileWriter('./log', sess.graph)\n",
        "  for epoch in range(epochs):\n",
        "    for x, y in zip(x_train, y_train):\n",
        "      sess.run(optimizer, feed_dict={X:x, Y:y})\n",
        "\n",
        "      summary_epochs=sess.run(merged_summary, feed_dict={X:x, Y:y})\n",
        "      writer.add_summary(summary_epochs, epoch)\n",
        "      if not epoch%40:\n",
        "        W1=sess.run(W)\n",
        "        B1=sess.run(B)\n",
        "        cost_iter=sess.run(cost_iteration, feed_dict={X:x, Y:y})\n",
        "        print('Epochs %f Cost %f Weight %f Bias %f' %(epoch, cost_iter, W1, B1))\n",
        "  Weight=sess.run(W)\n",
        "  Bias=sess.run(B)\n",
        "\n",
        "  plt.plot(x_train, y_train, 'o')\n",
        "  plt.plot(x_train, Weight*x_train+Bias)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "9JFl3qAEIN__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "H2qRniyXIgOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==1.15.5"
      ],
      "metadata": {
        "id": "z9jplfMDIg1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.zip"
      ],
      "metadata": {
        "id": "WTT2-e0FIkNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ngrok-v3-stable-linux-amd64.zip"
      ],
      "metadata": {
        "id": "xiHTrMG5Im5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "metadata": {
        "id": "iV3p0EfHIqY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "metadata": {
        "id": "HdN8qE0dIu_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "metadata": {
        "id": "XMs_xeQEIxWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "7m2bQ2jiI2Mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==1.15.5"
      ],
      "metadata": {
        "id": "Z79TsUq4I3M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "TLplT4Q3I59l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist_data = input_data.read_data_sets(\"./data\", one_hot = True)"
      ],
      "metadata": {
        "id": "NIPKomVDI9jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_training = mnist_data.train.num_examples\n",
        "num_testing = mnist_data.test.num_examples\n",
        "num_validation = mnist_data.validation.num_examples\n",
        "print(\"MNIST Datasize: Training samples: {0}, Testing samples: {1}\")"
      ],
      "metadata": {
        "id": "jHKDaAqcJIcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_input = 784\n",
        "n_hidden_1 = 512\n",
        "n_hidden_2 = 256\n",
        "n_hidden_3 = 128\n",
        "n_output = 10"
      ],
      "metadata": {
        "id": "TvwMv6aLJMue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-4\n",
        "epochs = 3000\n",
        "batch_size = 128\n",
        "keep_prob = tf.placeholder(tf.float32)"
      ],
      "metadata": {
        "id": "oqDInGd3JP5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(tf.float32, [None, n_input])\n",
        "Y = tf.placeholder(tf.float32, [None, n_output])"
      ],
      "metadata": {
        "id": "zzSnB72lJSTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_weight = {\"W1\": tf.Variable(tf.truncated_normal([n_input, n_hidden_1], stddev = 0.1)),\n",
        "             \"W2\": tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2], stddev = 0.1)),\n",
        "             \"W3\": tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3], stddev = 0.1)),\n",
        "             \"Wout\":tf.Variable(tf.truncated_normal([n_hidden_3, n_output]))\n",
        "}\n",
        "\n",
        "nn_bias = { \"B1\": tf.Variable(tf.truncated_normal([n_hidden_1])),\n",
        "            \"B2\": tf.Variable(tf.truncated_normal([n_hidden_2])),\n",
        "            \"B3\": tf.Variable(tf.truncated_normal([n_hidden_3])),\n",
        "            \"B4\": tf.Variable(tf.truncated_normal([n_output])),  \n",
        "           }"
      ],
      "metadata": {
        "id": "eNhAF1FfJVNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_layer_1 = tf.add(tf.matmul(X, nn_weight[\"W1\"]),nn_bias[\"B1\"])\n",
        "nn_layer_2 = tf.add(tf.matmul(nn_layer_1, nn_weight[\"W2\"]),nn_bias[\"B2\"])\n",
        "nn_layer_3 = tf.add(tf.matmul(nn_layer_2, nn_weight[\"W3\"]),nn_bias[\"B3\"])\n",
        "layer_drop = tf.nn.dropout(nn_layer_3, keep_prob)\n",
        "output_layer = tf.add(tf.matmul(layer_drop, nn_weight[\"Wout\"]), nn_bias[\"B4\"])"
      ],
      "metadata": {
        "id": "ifS1ZrkeJWhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "computed_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = output_layer, labels = Y))\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(computed_loss)\n",
        "\n",
        "prediction_out = tf.equal(tf.argmax(output_layer,1), tf.argmax(Y,1))\n",
        "\n",
        "nn_accuracy = tf.reduce_mean(tf.cast(prediction_out, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "saver = tf.train.Saver()"
      ],
      "metadata": {
        "id": "mDvMqsNuJaU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  for i in range(epochs):\n",
        "\n",
        "    mini_batch_x, mini_batch_y = mnist_data.train.next_batch(batch_size)\n",
        "    mini_batch_val_x, mini_batch_val_y = mnist_data.validation.next_batch(batch_size)\n",
        "\n",
        "    sess.run(optimizer, feed_dict = {X : mini_batch_x, Y : mini_batch_y, keep_prob:1})\n",
        "    \n",
        "    if i%100 == 0:\n",
        "      mini_batch_loss, mini_batch_accuracy = sess.run([computed_loss, nn_accuracy], feed_dict = {X: mini_batch_x, Y: mini_batch_y, keep_prob:1})\n",
        "\n",
        "      mini_batch_val_loss, mini_batch_val_accuracy = sess.run([computed_loss, nn_accuracy], feed_dict = {X: mini_batch_x, Y: mini_batch_y, keep_prob:1})\n",
        "\n",
        "      print(\"Iterations : {0} , Train_loss = {1}, Train_Accuracy {2}, Val_loss {3}, Val_accuracy {4}\".format(i, mini_batch_loss, mini_batch_accuracy, mini_batch_val_loss, mini_batch_val_accuracy))\n",
        "\n",
        "  print(\"Optimization Finished\")\n",
        "  test_accuracy = sess.run(nn_accuracy, feed_dict = {X:mnist_data.test.images, Y:mnist_data.test.labels, keep_prob:1.0})\n",
        "  print(\"Testing accuracy is {0}\".format(test_accuracy))\n",
        "\n",
        "  saver_path = saver.save(sess, \"./model/my_model.ckpt\")"
      ],
      "metadata": {
        "id": "nzg3xoCoJgxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread(\"7.jpg\")\n",
        "gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "rescaled_image = cv2.resize(gray_image, (28,28))\n",
        "plt.imshow(rescaled_image, cmap = 'gray')\n",
        "plt.show()\n",
        "rescaled_image.shape\n",
        "#test_image = rescaled_image.flatten()\n",
        "dum = rescaled_image.reshape(1,-1)/255\n",
        "dum.shape\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, \"./model/my_model.ckpt\")\n",
        "  Z = output_layer.eval(feed_dict = {X:dum, keep_prob:1.0})\n",
        "  y_pred = np.argmax(Z, axis = 1)\n",
        "  print(\"Prediction for test image is {0}\".format(y_pred))"
      ],
      "metadata": {
        "id": "4jx1XN6wJjl_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}